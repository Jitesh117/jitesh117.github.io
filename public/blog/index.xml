<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Jitesh117</title>
    <link>//localhost:1313/blog/</link>
    <description>Recent content on Jitesh117</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 25 May 2024 19:46:59 +0530</lastBuildDate>
    <atom:link href="//localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GruvBox dark</title>
      <link>//localhost:1313/blog/test/</link>
      <pubDate>Sat, 25 May 2024 19:46:59 +0530</pubDate>
      <guid>//localhost:1313/blog/test/</guid>
      <description>Heading one Heading two Heading three Heading four Heading five Heading Six import currentTheme from themes def displayTheme(): print(&amp;#34;Current Theme&amp;#34;) </description>
    </item>
    <item>
      <title>The Perceptron</title>
      <link>//localhost:1313/blog/perceptron/</link>
      <pubDate>Fri, 17 May 2024 15:43:31 +0530</pubDate>
      <guid>//localhost:1313/blog/perceptron/</guid>
      <description>The perceptron is a foundational concept in machine learning, representing the simplest type of artificial neural network. This article explores the perceptron model, its functionality, and its significance in the world of Machine learning.
Introduction to the Perceptron The perceptron, developed by Frank Rosenblatt in 1957, is a type of linear classifier used for binary classification tasks. It is the basis of more complex neural networks and deep learning models.</description>
    </item>
    <item>
      <title>The Curse of Dimensionality</title>
      <link>//localhost:1313/blog/the-curse-of-dimensionality/</link>
      <pubDate>Wed, 08 May 2024 15:15:29 +0530</pubDate>
      <guid>//localhost:1313/blog/the-curse-of-dimensionality/</guid>
      <description>As the number of features or dimensions grows in a dataset, the effectiveness of many machine learning algorithms can suffer from an issue called the &amp;ldquo;curse of dimensionality.&amp;rdquo; This problem is particularly prevalent for instance-based algorithms like K-nearest neighbors (KNN).
The KNN algorithm relies on calculating the distances between data points to determine which points are nearest neighbors.
In low dimensional spaces like 2D or 3D, the concepts of distance and neighborhood are relatively straightforward.</description>
    </item>
    <item>
      <title>Why to Use Tmux</title>
      <link>//localhost:1313/blog/why-to-use-tmux/</link>
      <pubDate>Tue, 30 Apr 2024 16:22:06 +0530</pubDate>
      <guid>//localhost:1313/blog/why-to-use-tmux/</guid>
      <description>1. Introduction tmux (Terminal Multiplexer) is a powerful terminal utility that allows you to create multiple terminal sessions, split windows, and detach and reattach sessions with ease. It&amp;rsquo;s a game-changer for anyone who works heavily in the terminal, providing a robust environment for multitasking, remote server management, and efficient workflow.
Benefits of using tmux Persistent sessions: Detach and reattach sessions, ensuring your work is never lost Efficient window and pane management: Split your terminal into multiple windows and panes Enhanced productivity: Streamline your workflow by running multiple commands simultaneously Remote session sharing: Share sessions with other users for collaboration or pair programming A Brief overview of its features tmux offers a wide range of features, including window and pane management, customizable status bars, key bindings, plugins, and more.</description>
    </item>
    <item>
      <title>Hierarchical Indexing in Pandas</title>
      <link>//localhost:1313/blog/hierarchical-indexing-in-pandas/</link>
      <pubDate>Thu, 25 Apr 2024 11:56:07 +0530</pubDate>
      <guid>//localhost:1313/blog/hierarchical-indexing-in-pandas/</guid>
      <description>Working with high-dimensional or categorized data can be a complex and challenging task, but Pandas provides a powerful tool to simplify this process: hierarchical indexing.
Hierarchical indexing, also known as multi-level or multi-indexed indexing, allows you to create and manipulate indexes with multiple levels or dimensions, providing a more intuitive and organized way to structure, access, and analyze your data.
This indexing technique is particularly useful when dealing with datasets that contain multiple levels of categorization or grouping, such as data from different regions, periods, or product categories.</description>
    </item>
    <item>
      <title>Activation Function and Loss Functions in Neural Networks</title>
      <link>//localhost:1313/blog/activation-function-and-loss-functions-in-neural-networks/</link>
      <pubDate>Fri, 19 Apr 2024 20:52:26 +0530</pubDate>
      <guid>//localhost:1313/blog/activation-function-and-loss-functions-in-neural-networks/</guid>
      <description>Activation Functions in Neural Networks Activation functions are a crucial component of artificial neural networks, serving as the mathematical operation that determines the output of a node or neuron. They introduce non-linearities to the network, allowing it to learn complex patterns and relationships within the data.
1. Sigmoid Function Sigmoid function, also known as the logistic function. This function squashes input values between 0 and 1, making it a popular choice for the output layer in binary classification problems.</description>
    </item>
    <item>
      <title>So You Think You Know Git?</title>
      <link>//localhost:1313/blog/so-you-think-you-know-git/</link>
      <pubDate>Mon, 15 Apr 2024 14:39:34 +0530</pubDate>
      <guid>//localhost:1313/blog/so-you-think-you-know-git/</guid>
      <description>This article is just about my learnings from the talk So you Think you know Git.
One might think that everything there is to know about Git has already been covered, but that would be wrong. Even today, the Git codebase is seeing around 9 commits per day and 10,000 commits in the last 3 years. That means there are still plenty of new things being added to Git that you might not know about.</description>
    </item>
    <item>
      <title>How I Learned to Type 150&#43; WPM</title>
      <link>//localhost:1313/blog/how-i-learned-to-type-fast/</link>
      <pubDate>Fri, 29 Mar 2024 16:25:00 +0530</pubDate>
      <guid>//localhost:1313/blog/how-i-learned-to-type-fast/</guid>
      <description>In this article I&amp;rsquo;m going to write about a topic very close to my heart, which is &amp;ldquo;How I Learned to type Faster&amp;rdquo;. Typing fast has been a game changer for me, and I believe anyone who types fast can become at least twice as productive as they already are.
But why bother to type fast? 1. It just really impresses people When people see me typing while coding, the single biggest compliment I&amp;rsquo;ve got is &amp;ldquo;Wow!</description>
    </item>
    <item>
      <title>Hash Tables Primer: The Ins and Outs of Efficient Key-Value Storage</title>
      <link>//localhost:1313/blog/the-ins-and-outs-of-hash-tables/</link>
      <pubDate>Tue, 26 Mar 2024 17:00:00 +0530</pubDate>
      <guid>//localhost:1313/blog/the-ins-and-outs-of-hash-tables/</guid>
      <description>Hash Tables are one of the most used Data structures, they&amp;rsquo;re so popular that almost every language has their own implementation and nomenclature of Hash Tables.
One very interesting fact about Hash Tables is that they&amp;rsquo;re also used as building blocks for:
Classes and its members Variable lookup table As we can see, Hash Tables are not just used in the business case of any application, but it&amp;rsquo;s also used in the inner working of any programming language.</description>
    </item>
    <item>
      <title>Hash Table Performance and Implementation</title>
      <link>//localhost:1313/blog/hash-table-performance-and-implementation/</link>
      <pubDate>Tue, 26 Mar 2024 15:34:47 +0530</pubDate>
      <guid>//localhost:1313/blog/hash-table-performance-and-implementation/</guid>
      <description>Hash Table gives constant time performance when there are zero collisions.
But that&amp;rsquo;s impossible!
So how can we quantify how &amp;ldquo;full&amp;rdquo; the table is? This is called the Load Factor
Load Factor Load Factor is the ratio of the total number of elements and the number of slots in the hash table.
Load Factor with different resolution strategies Load Factor with different resolution strategies With chaining, we can never fill the table i.</description>
    </item>
    <item>
      <title>Why I use Vim and why you should too</title>
      <link>//localhost:1313/blog/why-i-use-vim-and-why-you-should-too/</link>
      <pubDate>Wed, 20 Mar 2024 07:00:00 +0530</pubDate>
      <guid>//localhost:1313/blog/why-i-use-vim-and-why-you-should-too/</guid>
      <description>VIM or Vi Improved, is a free and open-source text editor for the terminal written by Bram Moolenaar. It is a highly powerful and versatile text editor that has managed to gather a devoted following among many people. Known for its efficiency, speed, and extensive customization options, Vim offers a unique modal editing approach that distinguishes it from traditional text editors.
Although I was familiar with Vim since 2020, I never thought of actually using it.</description>
    </item>
    <item>
      <title>Hyperparameter Tuning and Ensembling</title>
      <link>//localhost:1313/blog/ensembling-and-hyperparameter-tuning/</link>
      <pubDate>Sat, 16 Mar 2024 17:56:43 +0530</pubDate>
      <guid>//localhost:1313/blog/ensembling-and-hyperparameter-tuning/</guid>
      <description>Ensembling, a powerful technique in machine learning, has gained widespread popularity for its ability to significantly enhance predictive performance. By combining the predictions of multiple individual models, ensembles can often achieve better results than any single model alone. However, to fully leverage the potential of ensembling, it&amp;rsquo;s crucial to fine-tune the hyperparameters of the underlying base models.
Hyperparameter tuning involves searching for the optimal combination of model parameters that maximizes performance metrics such as accuracy or F1 score.</description>
    </item>
  </channel>
</rss>
