<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Activation Function and Loss Functions in Neural Networks | Jitesh117</title>
<meta name="keywords" content="">
<meta name="description" content="Activation Functions in Neural Networks Activation functions are a crucial component of artificial neural networks, serving as the mathematical operation that determines the output of a node or neuron. They introduce non-linearities to the network, allowing it to learn complex patterns and relationships within the data.
1. Sigmoid Function Sigmoid function, also known as the logistic function. This function squashes input values between 0 and 1, making it a popular choice for the output layer in binary classification problems.">
<meta name="author" content="">
<link rel="canonical" href="//localhost:1313/blog/activation-function-and-loss-functions-in-neural-networks/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d304b4aaad4543ab1fd32494a8e98b394bb751bda635acb81e2292240485743b.css" integrity="sha256-0wS0qq1FQ6sf0ySUqOmLOUu3Ub2mNay4HiKSJASFdDs=" rel="preload stylesheet" as="style">
<link rel="icon" href="//localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="//localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="//localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="//localhost:1313/blog/activation-function-and-loss-functions-in-neural-networks/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>


</head>

<body class=" dark" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="//localhost:1313/" accesskey="h" title="Jitesh117 (Alt + H)">Jitesh117</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="//localhost:1313/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/blog/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/musings/" title="Essays">
                    <span>Essays</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/shelf/" title="Shelf">
                    <span>Shelf</span>
                </a>
            </li>
        </ul>
    </nav>
    
    
</header><main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Activation Function and Loss Functions in Neural Networks
    </h1>
    <div class="post-meta"><span title='2024-04-19 20:52:26 +0530 IST'>April 19, 2024</span>&nbsp;Â·&nbsp;4 min

</div>
  </header> 
<figure class="entry-cover"><a href="//localhost:1313/images/graphs.png" target="_blank"
            rel="noopener noreferrer"><img loading="eager" src="//localhost:1313/images/graphs.png" alt=""></a>
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#activation-functions-in-neural-networks" aria-label="Activation Functions in Neural Networks">Activation Functions in Neural Networks</a><ul>
                        
                <li>
                    <a href="#1-sigmoid-function" aria-label="1. Sigmoid Function">1. Sigmoid Function</a></li>
                <li>
                    <a href="#2-relu-rectified-linear-unit" aria-label="2. ReLU (Rectified Linear Unit)">2. ReLU (Rectified Linear Unit)</a></li>
                <li>
                    <a href="#3-tanh-function" aria-label="3. Tanh Function">3. Tanh Function</a></li>
                <li>
                    <a href="#4-softmax-function" aria-label="4. Softmax Function">4. Softmax Function</a></li>
                <li>
                    <a href="#5-leaky-relu" aria-label="5. Leaky ReLU">5. Leaky ReLU</a></li>
                <li>
                    <a href="#6-elu-exponential-linear-unit" aria-label="6. ELU (Exponential Linear Unit)">6. ELU (Exponential Linear Unit)</a></li></ul>
                </li>
                <li>
                    <a href="#loss-functions-in-machine-learning" aria-label="Loss Functions in Machine Learning">Loss Functions in Machine Learning</a><ul>
                        
                <li>
                    <a href="#mean-squared-error-mse" aria-label="Mean Squared Error (MSE)">Mean Squared Error (MSE)</a></li>
                <li>
                    <a href="#binary-cross-entropy-loss" aria-label="Binary Cross-Entropy Loss">Binary Cross-Entropy Loss</a></li>
                <li>
                    <a href="#categorical-cross-entropy-loss" aria-label="Categorical Cross-Entropy Loss">Categorical Cross-Entropy Loss</a></li>
                <li>
                    <a href="#hinge-loss-for-svm" aria-label="Hinge Loss (for SVM)">Hinge Loss (for SVM)</a></li>
                <li>
                    <a href="#huber-loss" aria-label="Huber Loss">Huber Loss</a></li>
                <li>
                    <a href="#custom-loss-functions" aria-label="Custom Loss Functions">Custom Loss Functions</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="activation-functions-in-neural-networks">Activation Functions in Neural Networks<a hidden class="anchor" aria-hidden="true" href="#activation-functions-in-neural-networks">#</a></h2>
<p>Activation functions are a crucial component of artificial neural networks, serving as the mathematical operation that determines the output of a node or neuron. They introduce non-linearities to the network, allowing it to learn complex patterns and relationships within the data.</p>
<h3 id="1-sigmoid-function">1. Sigmoid Function<a hidden class="anchor" aria-hidden="true" href="#1-sigmoid-function">#</a></h3>
<p>Sigmoid function, also known as the logistic function. This function squashes input values between 0 and 1, making it a popular choice for the output layer in binary classification problems. Here&rsquo;s what it looks like mathematically:</p>
<p>$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$</p>
<p>And in Python:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))
</span></span></code></pre></div><h3 id="2-relu-rectified-linear-unit">2. ReLU (Rectified Linear Unit)<a hidden class="anchor" aria-hidden="true" href="#2-relu-rectified-linear-unit">#</a></h3>
<p>ReLU is one of the most popular activation functions in deep learning. It replaces all negative values in the input with zero, while leaving positive values unchanged.</p>
<p>Mathematically, the ReLU function is defined as:</p>
<p>$$ f(x) = \max(0, x) $$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">relu</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>maximum(<span style="color:#ae81ff">0</span>, x)
</span></span></code></pre></div><h3 id="3-tanh-function">3. Tanh Function<a hidden class="anchor" aria-hidden="true" href="#3-tanh-function">#</a></h3>
<p>Tanh (Hyperbolic Tangent) is like the sigmoid function&rsquo;s cooler, more balanced sibling. It squashes input values between -1 and 1, offering a symmetric range of outputs.</p>
<p>Mathematically, the tanh function is defined as:</p>
<p>$$ \text{tanh}(x) = \frac{e^x - e^{-x}}{e^{x + e^{-x}}}$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tanh</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>tanh(x)
</span></span></code></pre></div><h3 id="4-softmax-function">4. Softmax Function<a hidden class="anchor" aria-hidden="true" href="#4-softmax-function">#</a></h3>
<p>When you&rsquo;re dealing with multi-class classification problems, the softmax function is the one you want to call. This function converts raw scores into probabilities, ensuring that the sum of all probabilities equals 1.</p>
<p>Mathematically, the softmax function is defined as:</p>
<p>$$
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax</span>(x):
</span></span><span style="display:flex;"><span>    exp_x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(x <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>max(x))  <span style="color:#75715e"># For numerical stability</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> exp_x <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sum(exp_x, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><h3 id="5-leaky-relu">5. Leaky ReLU<a hidden class="anchor" aria-hidden="true" href="#5-leaky-relu">#</a></h3>
<p>Leaky ReLU is like ReLU&rsquo;s rebellious cousin. Instead of shutting down completely for negative inputs, it allows a small, non-zero gradient to flow through. This helps to address the dreaded &ldquo;dying ReLU&rdquo; problem, keeping the party going even when things get a little negative.</p>
<p>Mathematically, the leaky ReLU function is defined as:</p>
<p>$$ f(x) = \begin{cases} x &amp; \text{if } x &gt; 0 \ \alpha x &amp; \text{otherwise} \end{cases}
$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">leaky_relu</span>(x, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>, x, alpha <span style="color:#f92672">*</span> x)
</span></span></code></pre></div><h3 id="6-elu-exponential-linear-unit">6. ELU (Exponential Linear Unit)<a hidden class="anchor" aria-hidden="true" href="#6-elu-exponential-linear-unit">#</a></h3>
<p>The ELU activation function is like the cool kid on the block, combining the best of both worlds â the robustness of ReLU and the ability to capture negative values. It&rsquo;s a smooth, continuous function that avoids the &ldquo;dying ReLU&rdquo; problem while still providing the beneficial properties of ReLU.</p>
<p>Mathematically, the ELU function is defined as:</p>
<p>$$ f(x) = \begin{cases} x &amp; \text{if } x &gt; 0 \ \alpha (e^{x} - 1) &amp; \text{otherwise} \end{cases} $$</p>
<p>And in Python:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">elu</span>(x, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>, x, alpha <span style="color:#f92672">*</span> (np<span style="color:#f92672">.</span>exp(x) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))
</span></span></code></pre></div><p>These are just a few examples of activation functions used in neural networks. Each activation function has its advantages and disadvantages, and the choice often depends on the specific problem being solved and empirical performance on validation data. Experimentation with different activation functions is crucial to finding the most suitable one for your neural network architecture and dataset.</p>
<h2 id="loss-functions-in-machine-learning">Loss Functions in Machine Learning<a hidden class="anchor" aria-hidden="true" href="#loss-functions-in-machine-learning">#</a></h2>
<p>Alright, now that we&rsquo;ve had our fun with activation functions, let&rsquo;s talk about loss functions, also known as cost functions or objective functions. These Functions are the ones that measure the difference between our model&rsquo;s predictions and the actual values, quantifying its performance and guiding the optimization process.</p>
<h3 id="mean-squared-error-mse">Mean Squared Error (MSE)<a hidden class="anchor" aria-hidden="true" href="#mean-squared-error-mse">#</a></h3>
<ul>
<li>Calculates the average squared difference between predicted and actual values.</li>
<li>Commonly used in regression problems.</li>
<li>Mathematically: $$ MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$</li>
</ul>
<h3 id="binary-cross-entropy-loss">Binary Cross-Entropy Loss<a hidden class="anchor" aria-hidden="true" href="#binary-cross-entropy-loss">#</a></h3>
<ul>
<li>Used in binary classification problems to measure the dissimilarity between predicted probabilities and actual binary labels.</li>
<li>Mathematically: $$ BCE = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)] $$</li>
</ul>
<h3 id="categorical-cross-entropy-loss">Categorical Cross-Entropy Loss<a hidden class="anchor" aria-hidden="true" href="#categorical-cross-entropy-loss">#</a></h3>
<ul>
<li>An extension of binary cross-entropy loss for multi-class classification problems.</li>
<li>Measures the dissimilarity between predicted class probabilities and one-hot encoded target labels.</li>
<li>Mathematically: $$ CCE = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{m} y_{ij} \log(\hat{y}_{ij}) $$</li>
</ul>
<h3 id="hinge-loss-for-svm">Hinge Loss (for SVM)<a hidden class="anchor" aria-hidden="true" href="#hinge-loss-for-svm">#</a></h3>
<ul>
<li>Used in support vector machines (SVM) for binary classification.</li>
<li>Penalizes misclassified examples based on their distance from the decision boundary.</li>
<li>Mathematically: $$ HingeLoss = \max(0, 1 - y_i \cdot \hat{y}_i) $$</li>
</ul>
<h3 id="huber-loss">Huber Loss<a hidden class="anchor" aria-hidden="true" href="#huber-loss">#</a></h3>
<ul>
<li>Combines the advantages of squared error loss and absolute error loss.</li>
<li>Less sensitive to outliers compared to MSE.</li>
<li>Mathematically: $$ HuberLoss = \begin{cases} \frac{1}{2}(y - \hat{y})^2 &amp; \text{if } |y - \hat{y}| \leq \delta \ \delta(|y - \hat{y}| - \frac{1}{2}\delta) &amp; \text{otherwise} \end{cases} $$</li>
</ul>
<h3 id="custom-loss-functions">Custom Loss Functions<a hidden class="anchor" aria-hidden="true" href="#custom-loss-functions">#</a></h3>
<ul>
<li>In some cases, you might need to roll up your sleeves and craft your own custom loss functions tailored to your specific problem requirements.</li>
<li>Just make sure these functions are differentiable to facilitate gradient-based optimization.</li>
</ul>
<p>Loss functions are selected based on the nature of the problem, the type of output, and the desired properties of the model&rsquo;s predictions. Choosing an appropriate loss function is crucial for achieving optimal performance and training stability.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="//localhost:1313/blog/hierarchical-indexing-in-pandas/">
    <span class="title">Â« Prev</span>
    <br>
    <span>Hierarchical Indexing in Pandas</span>
  </a>
  <a class="next" href="//localhost:1313/blog/so-you-think-you-know-git/">
    <span class="title">Next Â»</span>
    <br>
    <span>So You Think You Know Git?</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Activation Function and Loss Functions in Neural Networks on x"
            href="https://x.com/intent/tweet/?text=Activation%20Function%20and%20Loss%20Functions%20in%20Neural%20Networks&amp;url=%2f%2flocalhost%3a1313%2fblog%2factivation-function-and-loss-functions-in-neural-networks%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Activation Function and Loss Functions in Neural Networks on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2f%2flocalhost%3a1313%2fblog%2factivation-function-and-loss-functions-in-neural-networks%2f&amp;title=Activation%20Function%20and%20Loss%20Functions%20in%20Neural%20Networks&amp;summary=Activation%20Function%20and%20Loss%20Functions%20in%20Neural%20Networks&amp;source=%2f%2flocalhost%3a1313%2fblog%2factivation-function-and-loss-functions-in-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Activation Function and Loss Functions in Neural Networks on reddit"
            href="https://reddit.com/submit?url=%2f%2flocalhost%3a1313%2fblog%2factivation-function-and-loss-functions-in-neural-networks%2f&title=Activation%20Function%20and%20Loss%20Functions%20in%20Neural%20Networks">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Activation Function and Loss Functions in Neural Networks on facebook"
            href="https://facebook.com/sharer/sharer.php?u=%2f%2flocalhost%3a1313%2fblog%2factivation-function-and-loss-functions-in-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Activation Function and Loss Functions in Neural Networks on whatsapp"
            href="https://api.whatsapp.com/send?text=Activation%20Function%20and%20Loss%20Functions%20in%20Neural%20Networks%20-%20%2f%2flocalhost%3a1313%2fblog%2factivation-function-and-loss-functions-in-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Activation Function and Loss Functions in Neural Networks on telegram"
            href="https://telegram.me/share/url?text=Activation%20Function%20and%20Loss%20Functions%20in%20Neural%20Networks&amp;url=%2f%2flocalhost%3a1313%2fblog%2factivation-function-and-loss-functions-in-neural-networks%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Activation Function and Loss Functions in Neural Networks on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Activation%20Function%20and%20Loss%20Functions%20in%20Neural%20Networks&u=%2f%2flocalhost%3a1313%2fblog%2factivation-function-and-loss-functions-in-neural-networks%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    
<script src="https://giscus.app/client.js" 
  data-repo="jitesh117/blog_comments"
  data-repo-id="R_kgDOLkuO3Q"
  data-category="Q&A"
  data-category-id="DIC_kwDOLkuO3c4CeMSc"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="0"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="en"
  data-loading="lazy"
  crossorigin="anonymous"
  async>
</script>

    <span>&copy; 2024 <a href="//localhost:1313/">Jitesh117</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
